{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Implementaci√≥n de al menos un algoritmo de recomendaci√≥n avanzado"
      ],
      "metadata": {
        "id": "DaZF9TWAReVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalando Dependencias"
      ],
      "metadata": {
        "id": "Q0Xv8Tto_22V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pVQjJoM_B5H"
      },
      "source": [
        "Instalando Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "F1Y1pRcivb5z",
        "outputId": "fc521fe6-3ea5-4d0a-eba6-2d1ae3ba300a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0Ô∏è‚É£   Install Java if not available\n",
            "‚úÖ Java is already installed.\n",
            "\n",
            "1Ô∏è‚É£   Download and install Hadoop and Spark\n",
            "‚úÖ https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz was found\n",
            "‚úÖ File ‚Äòspark-3.5.1-bin-hadoop3.tgz‚Äô already there; not retrieving.\n",
            "‚úÖ Folder already exists\n",
            "\n",
            "2Ô∏è‚É£   Start Spark engine\n",
            "‚úÖ stopping org.apache.spark.deploy.master.Master\n",
            "‚úÖ starting org.apache.spark.deploy.master.Master, logging to /content/spark-3.5.1-bin-hadoop3/logs/spark--org.apache.spark.deploy.master.Master-1-e851e84495b5.out\n",
            "‚úÖ stopping org.apache.spark.deploy.worker.Worker\n",
            "‚úÖ starting org.apache.spark.deploy.worker.Worker, logging to /content/spark-3.5.1-bin-hadoop3/logs/spark--org.apache.spark.deploy.worker.Worker-1-e851e84495b5.out\n",
            "\n",
            "3Ô∏è‚É£   Start Master Web UI\n",
            "Search for port number in log file /content/spark-3.5.1-bin-hadoop3/logs/spark--org.apache.spark.deploy.master.Master-1-e851e84495b5.out\n",
            "‚úÖ Master UI is available at localhost:8081 (attempt nr. 4)\n",
            "Click on the link below to open the Spark Web UI üöÄ\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8081, \"/\", \"https://localhost:8081/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4Ô∏è‚É£   Start history server\n",
            "‚úÖ stopping org.apache.spark.deploy.history.HistoryServer\n",
            "‚úÖ starting org.apache.spark.deploy.history.HistoryServer, logging to /content/spark-3.5.1-bin-hadoop3/logs/spark--org.apache.spark.deploy.history.HistoryServer-1-e851e84495b5.out\n",
            "Click on the link below to open the Spark History Server Web UI üöÄ\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(18080, \"/\", \"https://localhost:18080/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import socket\n",
        "import shutil\n",
        "import time\n",
        "import sys\n",
        "\n",
        "def run(cmd):\n",
        "    # run a shell command\n",
        "    try:\n",
        "        # Run the command and capture stdout and stderr\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        # Access stdout (stderr redirected to stdout)\n",
        "        stdout_result = subprocess_output.stdout.strip().splitlines()[-1]\n",
        "        # Process the results as needed\n",
        "        print(f'‚úÖ {stdout_result}')\n",
        "        return stdout_result\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(f\"Command failed with return code {e.returncode}\")\n",
        "        print(\"stdout:\", e.stdout)\n",
        "\n",
        "def is_java_installed():\n",
        "    return shutil.which(\"java\")\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "    os.environ['JAVA_HOME'] = ' /usr/lib/jvm/java-19-openjdk-amd64'\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(f'‚úÖ Done installing Java {java_version}')\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(f\"Command failed with return code {e.returncode}\")\n",
        "        print(\"stdout:\", e.stdout)\n",
        "\n",
        "print(\"\\n0Ô∏è‚É£   Install Java if not available\")\n",
        "if is_java_installed():\n",
        "    print(\"‚úÖ Java is already installed.\")\n",
        "else:\n",
        "    install_java()\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£   Download and install Hadoop and Spark\")\n",
        "# URL for downloading Hadoop and Spark\n",
        "SPARK_VERSION = \"3.5.1\"\n",
        "HADOOP_SPARK_URL = \"https://dlcdn.apache.org/spark/spark-\" + SPARK_VERSION + \\\n",
        "                   \"/spark-\" + SPARK_VERSION + \"-bin-hadoop3.tgz\"\n",
        "r = requests.head(HADOOP_SPARK_URL)\n",
        "if r.status_code >= 200 and r.status_code < 400:\n",
        "    print(f'‚úÖ {HADOOP_SPARK_URL} was found')\n",
        "else:\n",
        "    SPARK_CDN = \"https://dlcdn.apache.org/spark/\"\n",
        "    print(f'‚ö†Ô∏è {HADOOP_SPARK_URL} was NOT found. \\nCheck for available Spark versions in {SPARK_CDN}')\n",
        "\n",
        "# set some environment variables\n",
        "os.environ['SPARK_HOME'] = os.path.join(os.getcwd(), os.path.splitext(os.path.basename(HADOOP_SPARK_URL))[0])\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['SPARK_HOME'], 'bin'), os.environ['PATH']])\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['SPARK_HOME'], 'sbin'), os.environ['PATH']])\n",
        "\n",
        "# download Spark\n",
        "# using --no-clobber option will prevent wget from downloading file if already present\n",
        "# shell command: wget --no-clobber $HADOOP_SPARK_URL\n",
        "cmd = f\"wget --no-clobber {HADOOP_SPARK_URL}\"\n",
        "run(cmd)\n",
        "\n",
        "# uncompress\n",
        "try:\n",
        "    # Run the command and capture stdout and stderr\n",
        "    cmd = \"([ -d $(basename {0}|sed 's/\\.[^.]*$//') ] && echo -n 'Folder already exists') || (tar xzf $(basename {0}) && echo 'Uncompressed Spark distribution')\"\n",
        "    subprocess_output = subprocess.run(cmd.format(HADOOP_SPARK_URL), shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    # Access stdout (stderr redirected to stdout)\n",
        "    stdout_result = subprocess_output.stdout\n",
        "    # Process the results as needed\n",
        "    print(f'‚úÖ {stdout_result}')\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    # Handle the error if the command returns a non-zero exit code\n",
        "    print(f\"Command failed with return code {e.returncode}\")\n",
        "    print(\"stdout:\", e.stdout)\n",
        "\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£   Start Spark engine\")\n",
        "# start master\n",
        "# shell command: $SPARK_HOME/sbin/start-master.sh\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'stop-master.sh')\n",
        "run(cmd)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'start-master.sh')\n",
        "out = run(cmd)\n",
        "\n",
        "# start one worker (first stop it in case it's already running)\n",
        "# shell command: $SPARK_HOME/sbin/start-worker.sh spark://${HOSTNAME}:7077\n",
        "cmd = [os.path.join(os.environ['SPARK_HOME'], 'sbin', 'stop-worker.sh')]\n",
        "run(cmd)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'start-worker.sh') + ' ' + 'spark://'+socket.gethostname()+':7077'\n",
        "run(cmd)\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£   Start Master Web UI\")\n",
        "# get master UI's port number\n",
        "# the subprocess that's starting the master with start-master.sh\n",
        "# might still not be ready with assigning the port number at this point\n",
        "# therefore we check the logfile a few times (attempts=5) to see if the port\n",
        "# has been assigned. This might take 1-2 seconds.\n",
        "\n",
        "master_log = out.partition(\"logging to\")[2].strip()\n",
        "print(\"Search for port number in log file {}\".format(master_log))\n",
        "attempts = 10\n",
        "search_pattern = \"Successfully started service 'MasterUI' on port (\\d+)\"\n",
        "found = False\n",
        "for i in range(attempts):\n",
        "  if not found:\n",
        "   with open(master_log) as log:\n",
        "      found = re.search(search_pattern, log.read())\n",
        "      if found:\n",
        "          webUIport = found.group(1)\n",
        "          print(f\"‚úÖ Master UI is available at localhost:{webUIport} (attempt nr. {i})\")\n",
        "          break\n",
        "      else:\n",
        "          time.sleep(2) # need to try until port information is found in the logfile\n",
        "          i+=1\n",
        "if not found:\n",
        "  print(\"Could not find port for Master Web UI\\n\")\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    # serve the Web UI on Colab\n",
        "    print(\"Click on the link below to open the Spark Web UI üöÄ\")\n",
        "    from google.colab import output\n",
        "    output.serve_kernel_port_as_window(webUIport)\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£   Start history server\")\n",
        "# start history server\n",
        "# shell command: mkdir -p /tmp/spark-events\n",
        "# shell command: $SPARK_HOME/sbin/start-history-server.sh\n",
        "spark_events_dir = os.path.join('/tmp', 'spark-events')\n",
        "if not os.path.exists(spark_events_dir):\n",
        "    os.mkdir(spark_events_dir)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'stop-history-server.sh')\n",
        "run(cmd)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'start-history-server.sh')\n",
        "run(cmd)\n",
        "\n",
        "if IN_COLAB:\n",
        "    # serve the History Server\n",
        "    print(\"Click on the link below to open the Spark History Server Web UI üöÄ\")\n",
        "    output.serve_kernel_port_as_window(18080)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "l8kmJakDEV_S",
        "outputId": "13b6aee3-337c-4d84-db3d-3ad00b6c7221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.4)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.7)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pD7Wr-eWUHO8",
        "outputId": "cab1eeb2-7945-47db-a92a-82baa4314df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Using cached pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=86b6b7b7dd50e003df41dcd8623eeb7f5188efbccb89b68a40577a37a6d5f6f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ko-mAhdWPzBl",
        "outputId": "e195b222-fab8-4190-a377-2ff96816a30e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install  petastorm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "i7rXkdyoP30L",
        "outputId": "4003f8f9-09ec-4424-f4db-c8c05f291c4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting petastorm\n",
            "  Downloading petastorm-0.12.1-py2.py3-none-any.whl (284 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.2.1 (from petastorm)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=3.0.0 (from petastorm)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from petastorm) (0.18.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from petastorm) (1.25.2)\n",
            "Requirement already satisfied: packaging>=15.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (24.0)\n",
            "Requirement already satisfied: pandas>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (2.0.3)\n",
            "Requirement already satisfied: psutil>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (5.9.5)\n",
            "Requirement already satisfied: pyspark>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (3.5.1)\n",
            "Requirement already satisfied: pyzmq>=14.0.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (24.0.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from petastorm) (14.0.2)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from petastorm) (1.16.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from petastorm) (2023.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.0->petastorm) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.0->petastorm) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.0->petastorm) (2024.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark>=2.1.0->petastorm) (0.10.9.7)\n",
            "Installing collected packages: diskcache, dill, petastorm\n",
            "Successfully installed dill-0.3.8 diskcache-5.6.3 petastorm-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9LwSZ4HXFaDx"
      },
      "outputs": [],
      "source": [
        "#import opendatasets as od\n",
        "import pandas as pd\n",
        "import pyspark\n",
        "import opendatasets as od\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as fn\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from google.colab import drive\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "DIR = \"/content/drive/MyDrive/BigData/Spotify_rec_sys\"\n",
        "os.chdir(DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OAZD7iDpTdYR",
        "outputId": "0ef77711-b640-4635-d996-2e6ec10e6a4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOYEaGu_w97s"
      },
      "outputs": [],
      "source": [
        "od.download(\n",
        "\t\"https://www.kaggle.com/datasets/andrewmvd/spotify-playlists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fUBQu1M3wVml"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Rec_Sys\") \\\n",
        "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
        "    .config(\"petastorm.spark.converter.parentCacheDirUrl\", \"file:///tmp/petastorm_cache\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "fG49foAjxMjD",
        "outputId": "a37ccb65-d294-43fe-b37c-93b01cd18ee7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7df6b3489780>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e851e84495b5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Rec_Sys</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funciones para guardar y cargar informaci√≥n preprocesada\n",
        "import pickle\n",
        "\n",
        "def save_dict_keys_to_pickle(dictionary, pickle_filename):\n",
        "    keys = list(dictionary.keys())\n",
        "    with open(pickle_filename, 'wb') as f:\n",
        "        pickle.dump(keys, f)\n",
        "\n",
        "def load_dict_keys_from_pickle(pickle_filename):\n",
        "    with open(pickle_filename, 'rb') as f:\n",
        "        keys = pickle.load(f)\n",
        "    return keys\n",
        "\n",
        "def save_dataframes_to_parquet(dictionary, folder_path):\n",
        "    for key, dataframe in dictionary.items():\n",
        "        dataframe.write.mode(\"overwrite\").parquet(f\"{folder_path}/{key}.parquet\")\n",
        "\n",
        "def save_df_to_parquet(dataframe, name, folder_path):\n",
        "        dataframe.write.mode(\"overwrite\").parquet(f\"{folder_path}/{name}.parquet\")\n",
        "\n",
        "def load_dataframes_from_parquet(keys, folder_path):\n",
        "    dataframes = {}\n",
        "    for key in keys:\n",
        "        dataframe = spark.read.parquet(f\"{folder_path}/{key}.parquet\")\n",
        "        dataframes[key] = dataframe\n",
        "    return dataframes\n",
        "\n",
        "\n",
        "working_path='parquets_20240526'"
      ],
      "metadata": {
        "id": "1bew3XVNgDO0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocesamiento"
      ],
      "metadata": {
        "id": "WfqD-JjsAzqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Limpieza de datos"
      ],
      "metadata": {
        "id": "PiDmEFrkBFYb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr8ED4XBxerM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aff2fa1-c640-43e8-a150-293a4b1fd2aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(user_id='9cc0cfd4d7d7885102480dd99e7a90d6', artistname='Elvis Costello', trackname='(The Angels Wanna Wear My) Red Shoes', playlistname='HARD ROCK 2010')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "df = spark.read.option(\"header\", \"true\").csv(\"spotify-playlists//spotify_dataset.csv\")\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "df = df.toDF(*[col.replace(' ', '').replace('\"', '') for col in df.columns])\n",
        "\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Convirtiendo dataset a parquet"
      ],
      "metadata": {
        "id": "9N-v88skR81w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_df_to_parquet(df,'input_df', working_path)"
      ],
      "metadata": {
        "id": "3Vf8nB-HRA-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_df = spark.read.parquet(f\"{working_path}/input_df.parquet\")\n",
        "in_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MORzuYvyRqxg",
        "outputId": "9afb0510-b4d0-4b73-ffba-2293c8430532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(user_id='07f0fc3be95dcd878966b1f9572ff670', artistname='Miles Davis', trackname='Duke Booty', playlistname='Chill out')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creando dimensiones separadas para playlists, artistas, canciones y usuarios, reconstruyendo la matriz de reproducciones con los indices de dichas dimensiones."
      ],
      "metadata": {
        "id": "XNVXXQgCBMh8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19w5BaphaqGx"
      },
      "outputs": [],
      "source": [
        "dfs={}\n",
        "\n",
        "def df_dim(df, input_col):\n",
        "    windowSpec = Window.orderBy(input_col)\n",
        "    dfs[input_col]=df.select(input_col).distinct().withColumn(f\"{input_col}_index\", fn.row_number().over(windowSpec))\n",
        "\n",
        "for col_name in in_df.columns:\n",
        "    df_dim(df,col_name)\n",
        "\n",
        "newdf=df\n",
        "\n",
        "for i in range(0, len(df.columns)):\n",
        "    col_name = df.columns[i]\n",
        "    newdf=newdf.join(dfs[col_name].withColumnRenamed(col_name, col_name+'_base'), fn.col(col_name)==fn.col(col_name+'_base')).drop(col_name).drop(col_name+'_base')\n",
        "\n",
        "\n",
        "dfs['data'] = newdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving and reloading to work with parquet files\n",
        "save_dict_keys_to_pickle(dfs, 'keys.pkl')\n",
        "save_dataframes_to_parquet(dfs,working_path)\n",
        "loaded_keys = load_dict_keys_from_pickle('keys.pkl')\n",
        "dfs = load_dataframes_from_parquet(loaded_keys, working_path)"
      ],
      "metadata": {
        "id": "rwBwHoE_LDXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_reproductions =  dfs['data'].count()\n",
        "\n",
        "total_reproductions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36OWD4nVTB4H",
        "outputId": "43ff466c-7870-478b-e09b-089c5119aaed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12868518"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Se mantienen artistas con mas de 10000 reproducciones"
      ],
      "metadata": {
        "id": "NLDDqpYmTCwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "artist_reproductions = dfs['data'].groupBy(\"artistname_index\").agg(fn.count(\"*\").alias(\"total_reproductions\"))"
      ],
      "metadata": {
        "id": "0gzBzzzlKiG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artistas_mas_de_10k_reproducciones = artist_reproductions.filter(fn.col(\"total_reproductions\") > 10000)\n",
        "\n",
        "artistas_indices = artistas_mas_de_10k_reproducciones.select(\"artistname_index\")\n",
        "\n",
        "rep_top_artists = dfs['data'].join(artistas_indices, \"artistname_index\", \"inner\")"
      ],
      "metadata": {
        "id": "vLogFzqmBX7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rep_top_artists.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nkqlHk3VcF0",
        "outputId": "7655270e-9ab8-4777-a3e9-8a28ef07d79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+---------------+------------------+\n",
            "|artistname_index|user_id_index|trackname_index|playlistname_index|\n",
            "+----------------+-------------+---------------+------------------+\n",
            "|          258175|         6734|          45362|             53455|\n",
            "|          258175|         6734|          57234|             53455|\n",
            "|          258175|        15667|         196000|             88292|\n",
            "|          258175|          355|         196000|             76180|\n",
            "|          258175|        14817|         196000|             38211|\n",
            "|          258175|         6734|         196000|             53455|\n",
            "|          258175|         8561|         196000|            140025|\n",
            "|          258175|        14137|         196000|             13980|\n",
            "|          258175|         6498|         196003|            158500|\n",
            "|          258175|         5574|         196003|            117620|\n",
            "|          258175|        15611|         196003|            119615|\n",
            "|          258175|         5371|         196003|            127510|\n",
            "|          258175|         9708|         196003|             76432|\n",
            "|          258175|         8608|         196003|             21875|\n",
            "|          258175|        15063|         196003|            158162|\n",
            "|          258175|          878|         196003|            127510|\n",
            "|          258175|         9000|         196003|            118985|\n",
            "|          258175|         3124|         196003|             94045|\n",
            "|          258175|         5559|         196003|            127529|\n",
            "|          258175|         9336|         196003|            127529|\n",
            "+----------------+-------------+---------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rep_top_artists.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaCWjPYrBdpO",
        "outputId": "7f500e33-b770-4d93-93b9-a47af96b6620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1881913"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_artists = dfs['artistname'].join(artist_reproductions.orderBy(fn.col(\"total_reproductions\").desc()).limit(10),'artistname_index')\n",
        "print(\"Top 10 Artists:\")\n",
        "top_10_artists.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgt_9Ov0CNZ_",
        "outputId": "1ef25452-1a78-4db8-85be-27ccf6f4c585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Artists:\n",
            "+----------------+------------------+-------------------+\n",
            "|artistname_index|        artistname|total_reproductions|\n",
            "+----------------+------------------+-------------------+\n",
            "|           51082|          Coldplay|              35485|\n",
            "|           59560|         Daft Punk|              36086|\n",
            "|           63174|       David Bowie|              27802|\n",
            "|           80896|            Eminem|              28896|\n",
            "|          116342|             JAY Z|              28928|\n",
            "|          133453|        Kanye West|              29111|\n",
            "|          169467|   Michael Jackson|              26336|\n",
            "|          203157|             Queen|              28079|\n",
            "|          204527|         Radiohead|              31429|\n",
            "|          257149|The Rolling Stones|              30832|\n",
            "+----------------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfs['data_top_artists'] = rep_top_artists"
      ],
      "metadata": {
        "id": "UeuGgngYOJQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs['data_top_artists'].select(\"artistname_index\").distinct().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTMDB3cK_h2p",
        "outputId": "2aaa8e05-0043-487c-bba7-1883b0483fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "117"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dict_keys_to_pickle(dfs, 'keys.pkl')\n",
        "save_df_to_parquet(dfs['data_top_artists'],'data_top_artists', working_path)"
      ],
      "metadata": {
        "id": "tTJIMONrWRE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matriz de reporducciones seg√∫n el artista y normalizaci√≥n de los datos."
      ],
      "metadata": {
        "id": "rwyEmsfrBjjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q4d3ht_cAhi"
      },
      "outputs": [],
      "source": [
        "counts_df = dfs['data_top_artists'].groupBy(\"user_id_index\", \"artistname_index\").agg(fn.count(\"*\").alias(\"reproductions\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TETF7Ha-cjqS",
        "outputId": "b589b309-91cc-4e85-92fa-1dd6726740db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(user_id_index=6734, artistname_index=258175, reproductions=128, normalized_reproduction=0.03796711509715994),\n",
              " Row(user_id_index=15667, artistname_index=258175, reproductions=70, normalized_reproduction=0.02062780269058296),\n",
              " Row(user_id_index=355, artistname_index=258175, reproductions=58, normalized_reproduction=0.017040358744394617),\n",
              " Row(user_id_index=14817, artistname_index=258175, reproductions=99, normalized_reproduction=0.02929745889387145),\n",
              " Row(user_id_index=8561, artistname_index=258175, reproductions=16, normalized_reproduction=0.004484304932735426),\n",
              " Row(user_id_index=14137, artistname_index=258175, reproductions=48, normalized_reproduction=0.014050822122571001),\n",
              " Row(user_id_index=6498, artistname_index=258175, reproductions=314, normalized_reproduction=0.09357249626307922),\n",
              " Row(user_id_index=5574, artistname_index=258175, reproductions=113, normalized_reproduction=0.03348281016442452),\n",
              " Row(user_id_index=15611, artistname_index=258175, reproductions=171, normalized_reproduction=0.05082212257100149),\n",
              " Row(user_id_index=5371, artistname_index=258175, reproductions=106, normalized_reproduction=0.03139013452914798)]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "max_reproduction = counts_df.agg({\"reproductions\": \"max\"}).collect()[0][0]\n",
        "min_reproduction = counts_df.agg({\"reproductions\": \"min\"}).collect()[0][0]\n",
        "\n",
        "normalized_pl_counts_df = counts_df.withColumn(\"normalized_reproduction\", (fn.col(\"reproductions\") - min_reproduction) / (max_reproduction - min_reproduction))\n",
        "\n",
        "dfs['norm_data_top_artists'] = normalized_pl_counts_df\n",
        "\n",
        "normalized_pl_counts_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dict_keys_to_pickle(dfs, 'keys.pkl')\n",
        "save_df_to_parquet(dfs['norm_data_top_artists'], 'norm_data_top_artists', working_path)"
      ],
      "metadata": {
        "id": "GcrCgNusPVYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cargando datos preprocesados"
      ],
      "metadata": {
        "id": "gqIjrILYZ8fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_keys = load_dict_keys_from_pickle('keys.pkl')\n",
        "dfs = load_dataframes_from_parquet(loaded_keys, working_path)"
      ],
      "metadata": {
        "id": "Nv8r0Z1naBQx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, df in dfs.items():\n",
        "    print(k, df.head(10))"
      ],
      "metadata": {
        "id": "dBjrbYQSbaYQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "99089460-ffac-4280-ac5f-7a11125f95f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_id [Row(user_id='00055176fea33f6e027cd3302289378b', user_id_index=1), Row(user_id='0007f3dd09c91198371454c608d47f22', user_id_index=2), Row(user_id='000b0f32b5739f052b9d40fcc5c41079', user_id_index=3), Row(user_id='000c11a16c89aa4b14b328080f5954ee', user_id_index=4), Row(user_id='00123e0f544dee3ab006aa7f1e5725a7', user_id_index=5), Row(user_id='00139e9cb50fb309549e1561b476226d', user_id_index=6), Row(user_id='00152c870313100559aad7b097d9c1f5', user_id_index=7), Row(user_id='00154ec9dd1acd4ebfb521629dcb3948', user_id_index=8), Row(user_id='001599a07cb8ef5f114a9fcf4e0e2757', user_id_index=9), Row(user_id='0019363a0d57e94d39988c31eeb8d015', user_id_index=10)]\n",
            "artistname [Row(artistname=' Dolce', artistname_index=1), Row(artistname=' OneVoice', artistname_index=2), Row(artistname='!!!', artistname_index=3), Row(artistname='!!! (Chk Chk Chk)', artistname_index=4), Row(artistname='!!! Chk Chik Chick', artistname_index=5), Row(artistname='!ATTENTION!', artistname_index=6), Row(artistname='!DELADAP', artistname_index=7), Row(artistname='!Dela Dap', artistname_index=8), Row(artistname='!DelaDap', artistname_index=9), Row(artistname='!Distain', artistname_index=10)]\n",
            "trackname [Row(trackname=' \"Cachaito\" L√≥pez Y \"Guajiro\" Mirabal De Buena Vista Social Club Y Manuel \"Galb√°n\" Torralba\"', trackname_index=1), Row(trackname=' 15 Years of Tummy Touch Records in Dub', trackname_index=2), Row(trackname=' Alan Jackson', trackname_index=3), Row(trackname=' Ashley Tisdale', trackname_index=4), Row(trackname=' Ashley Tisdale & Lucas Grabeel\"', trackname_index=5), Row(trackname=' Babi Floyd\"', trackname_index=6), Row(trackname=' Babyface & Whitney Houston\"', trackname_index=7), Row(trackname=' Bert with The Whispering Orchestra\"', trackname_index=8), Row(trackname=' Bill Stepney\"', trackname_index=9), Row(trackname=' Bumblefoot', trackname_index=10)]\n",
            "playlistname [Row(playlistname=' ', playlistname_index=1), Row(playlistname='        waves', playlistname_index=2), Row(playlistname='  11', playlistname_index=3), Row(playlistname='  Frida', playlistname_index=4), Row(playlistname='  New tunes 05/11', playlistname_index=5), Row(playlistname=\"  You're the Worst\", playlistname_index=6), Row(playlistname='  joni mitchell       ', playlistname_index=7), Row(playlistname='  julia musica', playlistname_index=8), Row(playlistname=' !!', playlistname_index=9), Row(playlistname=' \"\"Appassionata\"\" - Allegro Assai \"\"', playlistname_index=10)]\n",
            "data [Row(user_id_index=15051, artistname_index=307, trackname_index=6, playlistname_index=100652), Row(user_id_index=5669, artistname_index=456, trackname_index=19, playlistname_index=869), Row(user_id_index=13756, artistname_index=29, trackname_index=20, playlistname_index=1924), Row(user_id_index=12151, artistname_index=459, trackname_index=23, playlistname_index=88917), Row(user_id_index=4852, artistname_index=77970, trackname_index=31, playlistname_index=70286), Row(user_id_index=11701, artistname_index=169, trackname_index=32, playlistname_index=2328), Row(user_id_index=9743, artistname_index=270, trackname_index=34, playlistname_index=703), Row(user_id_index=3808, artistname_index=270, trackname_index=34, playlistname_index=703), Row(user_id_index=4213, artistname_index=591, trackname_index=35, playlistname_index=1091), Row(user_id_index=8235, artistname_index=576, trackname_index=42, playlistname_index=28443)]\n",
            "data_top_artists [Row(artistname_index=258175, user_id_index=6734, trackname_index=45362, playlistname_index=53455), Row(artistname_index=258175, user_id_index=6734, trackname_index=57234, playlistname_index=53455), Row(artistname_index=258175, user_id_index=15667, trackname_index=196000, playlistname_index=88292), Row(artistname_index=258175, user_id_index=355, trackname_index=196000, playlistname_index=76180), Row(artistname_index=258175, user_id_index=14817, trackname_index=196000, playlistname_index=38211), Row(artistname_index=258175, user_id_index=6734, trackname_index=196000, playlistname_index=53455), Row(artistname_index=258175, user_id_index=8561, trackname_index=196000, playlistname_index=140025), Row(artistname_index=258175, user_id_index=14137, trackname_index=196000, playlistname_index=13980), Row(artistname_index=258175, user_id_index=6498, trackname_index=196003, playlistname_index=158500), Row(artistname_index=258175, user_id_index=5574, trackname_index=196003, playlistname_index=117620)]\n",
            "norm_data_top_artists [Row(user_id_index=6734, artistname_index=258175, reproductions=128, normalized_reproduction=0.03796711509715994), Row(user_id_index=15667, artistname_index=258175, reproductions=70, normalized_reproduction=0.02062780269058296), Row(user_id_index=355, artistname_index=258175, reproductions=58, normalized_reproduction=0.017040358744394617), Row(user_id_index=14817, artistname_index=258175, reproductions=99, normalized_reproduction=0.02929745889387145), Row(user_id_index=8561, artistname_index=258175, reproductions=16, normalized_reproduction=0.004484304932735426), Row(user_id_index=14137, artistname_index=258175, reproductions=48, normalized_reproduction=0.014050822122571001), Row(user_id_index=6498, artistname_index=258175, reproductions=314, normalized_reproduction=0.09357249626307922), Row(user_id_index=5574, artistname_index=258175, reproductions=113, normalized_reproduction=0.03348281016442452), Row(user_id_index=15611, artistname_index=258175, reproductions=171, normalized_reproduction=0.05082212257100149), Row(user_id_index=5371, artistname_index=258175, reproductions=106, normalized_reproduction=0.03139013452914798)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividiendo el dataset segun el numero de usuarios distintos\n",
        "\n",
        "unique_usr_ids =  dfs['norm_data_top_artists'].select(\"user_id_index\").distinct().orderBy(\"user_id_index\")\n",
        "\n",
        "\n",
        "train_usr_ids, test_usr_ids = unique_usr_ids.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "\n",
        "train_data = dfs['norm_data_top_artists'].join(train_usr_ids, on=\"user_id_index\", how=\"inner\")\n",
        "test_data = dfs['norm_data_top_artists'].join(test_usr_ids, on=\"user_id_index\", how=\"inner\")\n",
        "\n",
        "print(\"Training DataFrame:\")\n",
        "train_data.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YXRvHm0IA9VE",
        "outputId": "ae922ff1-fc01-49ff-8860-7fb01594ce3f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DataFrame:\n",
            "+-------------+----------------+-------------+-----------------------+\n",
            "|user_id_index|artistname_index|reproductions|normalized_reproduction|\n",
            "+-------------+----------------+-------------+-----------------------+\n",
            "|         6734|          258175|          128|    0.03796711509715994|\n",
            "|        15667|          258175|           70|    0.02062780269058296|\n",
            "|        14817|          258175|           99|    0.02929745889387145|\n",
            "|         8561|          258175|           16|   0.004484304932735426|\n",
            "|        14137|          258175|           48|   0.014050822122571001|\n",
            "|         6498|          258175|          314|    0.09357249626307922|\n",
            "|         5574|          258175|          113|    0.03348281016442452|\n",
            "|        15611|          258175|          171|    0.05082212257100149|\n",
            "|         5371|          258175|          106|    0.03139013452914798|\n",
            "|         9708|          258175|           36|    0.01046337817638266|\n",
            "|        15063|          258175|          106|    0.03139013452914798|\n",
            "|          878|          258175|          160|    0.04753363228699552|\n",
            "|         9000|          258175|          107|   0.031689088191330345|\n",
            "|         3124|          258175|           89|   0.026307922272047833|\n",
            "|         5559|          258175|          109|    0.03228699551569507|\n",
            "|         9336|          258175|          165|    0.04902840059790733|\n",
            "|          289|          258175|          106|    0.03139013452914798|\n",
            "|        10098|          258175|           78|   0.023019431988041853|\n",
            "|         4538|          258175|          170|    0.05052316890881913|\n",
            "|        10063|          258175|          106|    0.03139013452914798|\n",
            "+-------------+----------------+-------------+-----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg4ZykrHWRZN",
        "outputId": "8b6c2020-c540-4cd9-e3d7-9a80d489eb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------------+-------------+-----------------------+\n",
            "|user_id_index|artistname_index|reproductions|normalized_reproduction|\n",
            "+-------------+----------------+-------------+-----------------------+\n",
            "|         5803|          285680|            4|   8.968609865470852E-4|\n",
            "|         5803|          285075|            5|   0.001195814648729447|\n",
            "|         5803|          278507|          221|    0.06576980568011959|\n",
            "|         5803|          277472|           16|   0.004484304932735426|\n",
            "|         5803|          272596|            2|   2.989536621823617...|\n",
            "|         5803|          269254|            1|                    0.0|\n",
            "|         5803|          260511|            2|   2.989536621823617...|\n",
            "|         5803|          258829|           12|   0.003288490284005979|\n",
            "|         5803|          257149|            5|   0.001195814648729447|\n",
            "|         5803|          248402|            3|   5.979073243647235E-4|\n",
            "|         5803|          246063|          106|    0.03139013452914798|\n",
            "|         5803|          204527|           31|   0.008968609865470852|\n",
            "|         5803|          203664|           12|   0.003288490284005979|\n",
            "|         5803|          203240|            2|   2.989536621823617...|\n",
            "|         5803|          169467|            9|   0.002391629297458894|\n",
            "|         5803|          168699|            4|   8.968609865470852E-4|\n",
            "|         5803|          164895|            1|                    0.0|\n",
            "|         5803|          157985|            2|   2.989536621823617...|\n",
            "|         5803|          138370|           20|   0.005680119581464873|\n",
            "|         5803|          126554|           37|   0.010762331838565023|\n",
            "+-------------+----------------+-------------+-----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = train_data.count()\n",
        "\n",
        "# Calculate unique counts\n",
        "n_users = train_data.select(\"user_id_index\").distinct().count()\n",
        "n_artists = train_data.select(\"artistname_index\").distinct().count()\n",
        "\n",
        "# Ensure the maximum index is within range\n",
        "max_user_id = train_data.select(\"user_id_index\").rdd.max()[0]\n",
        "max_artist_id = train_data.select(\"artistname_index\").rdd.max()[0]\n",
        "\n",
        "max_id_users = max(n_users, max_user_id + 1)\n",
        "max_id_artists = max(n_artists, max_artist_id + 1)\n",
        "\n",
        "print(f\"Max ID users: {max_id_users}\")\n",
        "print(f\"Max ID artists: {max_id_artists}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8HuObdFaMAdb",
        "outputId": "7d172cc3-90ed-45a1-a0ff-f087bd3ee516"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max ID users: 15915\n",
            "Max ID artists: 285681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
        "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
        "from petastorm.tf_utils import make_petastorm_dataset\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "q_nvM48kC-RY"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate steps per epoch\n",
        "batch_size = 2048\n",
        "steps_per_epoch = total_rows // batch_size\n",
        "epochs = 10\n",
        "embedding_size = 100"
      ],
      "metadata": {
        "id": "S6u0SW-kN_1j"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkDatasetConverter\n",
        "converter = make_spark_converter(train_data)\n",
        "\n",
        "\n",
        "user_input = Input(shape=(1,), name='user_input')\n",
        "user_embedding = Embedding(max_id_users, embedding_size, input_length=1, name='user_embedding')(user_input)\n",
        "user_vec = Flatten(name='user_flatten')(user_embedding)\n",
        "\n",
        "artist_input = Input(shape=(1,), name='artist_input')\n",
        "artist_embedding = Embedding(max_id_artists, embedding_size, input_length=1, name='artist_embedding')(artist_input)\n",
        "artist_vec = Flatten(name='artist_flatten')(artist_embedding)\n",
        "\n",
        "concat = Concatenate()([user_vec, artist_vec])\n",
        "dense1 = Dense(128, activation='relu')(concat)\n",
        "dense2 = Dense(64, activation='relu')(dense1)\n",
        "output = Dense(1)(dense2)\n",
        "\n",
        "model = Model([user_input, artist_input], output)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# Training with Petastorm dataset\n",
        "def transform_row(row):\n",
        "    return {\"user_input\": row.user_id_index, \"artist_input\": row.artistname_index}, row.normalized_reproduction\n",
        "\n",
        "# Create TensorFlow dataset\n",
        "with converter.make_tf_dataset(batch_size=batch_size) as dataset:\n",
        "    dataset = dataset.map(lambda x: transform_row(x))\n",
        "\n",
        "    # Implement early stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=dataset, validation_steps=steps_per_epoch, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Hnuovud6Bp46",
        "outputId": "1837cfff-885f-460e-d187-38e2a3edace4"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
            "  self._filesystem = pyarrow.localfs\n",
            "WARNING:petastorm.spark.spark_dataset_converter:Converting floating-point columns to float32\n",
            "WARNING:petastorm.spark.spark_dataset_converter:The median size 472859 B (< 50 MB) of the parquet files is too small. Total size: 906227 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///tmp/petastorm_cache/20240526223056-appid-local-1716756405261-9ca118f7-89f9-4e97-b704-960552e5d1da/part-00000-92c93513-65b1-4219-8e07-d03531c25dd1-c000.parquet, ...\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/etl/dataset_metadata.py:402: FutureWarning: Passing 'use_legacy_dataset=True' to get the legacy behaviour is deprecated as of pyarrow 11.0.0, and the legacy implementation will be removed in a future version. The legacy behaviour was still chosen because a deprecated 'pyarrow.filesystem' filesystem was specified (use the filesystems from pyarrow.fs instead).\n",
            "  dataset = pq.ParquetDataset(path_or_paths, filesystem=fs, validate_schema=False, metadata_nthreads=10)\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/etl/dataset_metadata.py:402: FutureWarning: Specifying the 'metadata_nthreads' argument is deprecated as of pyarrow 8.0.0, and the argument will be removed in a future version\n",
            "  dataset = pq.ParquetDataset(path_or_paths, filesystem=fs, validate_schema=False, metadata_nthreads=10)\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/etl/dataset_metadata.py:362: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
            "  if not dataset.common_metadata:\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/reader.py:420: FutureWarning: Passing 'use_legacy_dataset=True' to get the legacy behaviour is deprecated as of pyarrow 11.0.0, and the legacy implementation will be removed in a future version. The legacy behaviour was still chosen because a deprecated 'pyarrow.filesystem' filesystem was specified (use the filesystems from pyarrow.fs instead).\n",
            "  self.dataset = pq.ParquetDataset(dataset_path, filesystem=pyarrow_filesystem,\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/reader.py:420: FutureWarning: Specifying the 'metadata_nthreads' argument is deprecated as of pyarrow 8.0.0, and the argument will be removed in a future version\n",
            "  self.dataset = pq.ParquetDataset(dataset_path, filesystem=pyarrow_filesystem,\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/unischema.py:317: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
            "  meta = parquet_dataset.pieces[0].get_metadata()\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/unischema.py:321: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
            "  for partition in (parquet_dataset.partitions or []):\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/etl/dataset_metadata.py:253: FutureWarning: 'ParquetDataset.metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
            "  metadata = dataset.metadata\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/etl/dataset_metadata.py:254: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
            "  common_metadata = dataset.common_metadata\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/etl/dataset_metadata.py:350: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
            "  futures_list = [thread_pool.submit(_split_piece, piece, dataset.fs.open) for piece in dataset.pieces]\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/etl/dataset_metadata.py:350: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
            "  futures_list = [thread_pool.submit(_split_piece, piece, dataset.fs.open) for piece in dataset.pieces]\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/etl/dataset_metadata.py:334: FutureWarning: ParquetDatasetPiece is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
            "  return [pq.ParquetDatasetPiece(piece.path, open_file_func=fs_open,\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/arrow_reader_worker.py:132: FutureWarning: Passing 'use_legacy_dataset=True' to get the legacy behaviour is deprecated as of pyarrow 11.0.0, and the legacy implementation will be removed in a future version. The legacy behaviour was still chosen because a deprecated 'pyarrow.filesystem' filesystem was specified (use the filesystems from pyarrow.fs instead).\n",
            "  self._dataset = pq.ParquetDataset(\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/arrow_reader_worker.py:140: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
            "  parquet_file = ParquetFile(self._dataset.fs.open(piece.path))\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/arrow_reader_worker.py:288: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
            "  partition_names = self._dataset.partitions.partition_names if self._dataset.partitions else set()\n",
            "/usr/local/lib/python3.10/dist-packages/petastorm/arrow_reader_worker.py:291: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
            "  table = piece.read(columns=column_names - partition_names, partitions=self._dataset.partitions)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "101/101 [==============================] - 87s 847ms/step - loss: 1.8578e-04 - val_loss: 7.0677e-05\n",
            "Epoch 2/10\n",
            "101/101 [==============================] - 82s 815ms/step - loss: 6.2150e-05 - val_loss: 6.1484e-05\n",
            "Epoch 3/10\n",
            "101/101 [==============================] - 86s 852ms/step - loss: 5.1474e-05 - val_loss: 4.9467e-05\n",
            "Epoch 4/10\n",
            "101/101 [==============================] - 82s 817ms/step - loss: 4.5729e-05 - val_loss: 4.7544e-05\n",
            "Epoch 5/10\n",
            "101/101 [==============================] - 84s 836ms/step - loss: 5.1317e-05 - val_loss: 4.0930e-05\n",
            "Epoch 6/10\n",
            "101/101 [==============================] - 82s 814ms/step - loss: 3.9329e-05 - val_loss: 3.8764e-05\n",
            "Epoch 7/10\n",
            "101/101 [==============================] - 84s 832ms/step - loss: 3.6409e-05 - val_loss: 4.5511e-05\n",
            "Epoch 8/10\n",
            "101/101 [==============================] - 83s 819ms/step - loss: 2.5931e-05 - val_loss: 3.7847e-05\n",
            "Epoch 9/10\n",
            "101/101 [==============================] - 83s 827ms/step - loss: 4.0968e-05 - val_loss: 3.2006e-05\n",
            "Epoch 10/10\n",
            "101/101 [==============================] - 83s 819ms/step - loss: 3.1517e-05 - val_loss: 3.0189e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = working_path + f\"//model_{epochs}_{batch_size}\""
      ],
      "metadata": {
        "id": "4l2RD6A-NPVW"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(model_save_path)"
      ],
      "metadata": {
        "id": "Ov_uGmnaQp07"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model(model_save_path)"
      ],
      "metadata": {
        "id": "CkIZoEGQqvQT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para evaluar este modelo se utillizaran 3 metricas:\n",
        "- Presision@K\n",
        "- Mean Average Precision (MAP)\n",
        "- Normalized Discounted Cumulative Gain (NDCG)"
      ],
      "metadata": {
        "id": "wIIGMoMoVURE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_top_k_artists(user_id_index, model, data, k=10):\n",
        "    artist_ids = data[\"artistname_index\"].unique()\n",
        "    user_ids = [user_id_index] * len(artist_ids)\n",
        "    user_ids_array = np.array(user_ids)\n",
        "    artist_ids_array = np.array(artist_ids)\n",
        "    predictions = model.predict([user_ids_array, artist_ids_array], verbose=0)\n",
        "    predicted_scores = list(zip(artist_ids, predictions))\n",
        "    top_k_artists = sorted(predicted_scores, key=lambda x: x[1], reverse=True)[:k]\n",
        "    top_k_artist_ids = [artist_id for artist_id, score in top_k_artists]\n",
        "    return top_k_artist_ids\n",
        "\n",
        "#Precision@K\n",
        "\n",
        "def precision_at_k(recommended_items, relevant_items, k):\n",
        "    relevant_set = set(relevant_items)\n",
        "    recommended_set = set(recommended_items)\n",
        "    return len(recommended_set & relevant_set) / k\n",
        "\n",
        "#MAP\n",
        "\n",
        "def average_precision(actual, predicted):\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "    return score / min(len(actual), len(predicted))\n",
        "\n",
        "def mean_average_precision(actual_items, predicted_items):\n",
        "    return np.mean([average_precision(actual, predicted) for actual, predicted in zip(actual_items, predicted_items)])\n",
        "\n",
        "# NDCG\n",
        "\n",
        "def dcg_at_k(r, k):\n",
        "    r = np.asfarray(r)[:k]\n",
        "    if r.size:\n",
        "        return np.sum(np.divide(np.power(2, r) - 1, np.log2(np.arange(2, r.size + 2))))\n",
        "    return 0.0\n",
        "\n",
        "def ndcg_at_k(r, k):\n",
        "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
        "    if not idcg:\n",
        "        return 0.0\n",
        "    return dcg_at_k(r, k) / idcg\n",
        "\n",
        "def normalized_discounted_cumulative_gain(actual, predicted, k):\n",
        "    r = [1 if p in actual else 0 for p in predicted]\n",
        "    return ndcg_at_k(r, k)"
      ],
      "metadata": {
        "id": "mES4x3MhTbbt"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(test_data, model, k=10):\n",
        "    user_ids = test_data['user_id_index'].unique()\n",
        "    avg_map = 0.0\n",
        "    avg_ndcg = 0.0\n",
        "    precision_scores = []\n",
        "\n",
        "    for user_id in user_ids:\n",
        "        relevant_items = test_data[test_data['user_id_index'] == user_id]['artistname_index'].tolist()\n",
        "\n",
        "        #Se ignoran ususarios que tengan pocos artistas (Arranque en fr√≠o)\n",
        "        if len(relevant_items) < 25:\n",
        "            continue\n",
        "\n",
        "        recommended_items = recommend_top_k_artists(user_id, model, test_data, k)\n",
        "\n",
        "\n",
        "        #Calcular Precision@K\n",
        "        precision = precision_at_k(recommended_items, relevant_items, k)\n",
        "        precision_scores.append(precision)\n",
        "\n",
        "        # Calcular MAP para el usuario actual\n",
        "        map_score = average_precision(relevant_items, recommended_items[:k])\n",
        "        avg_map += map_score\n",
        "\n",
        "        # Calcular NDCG para el usuario actual\n",
        "        ndcg_score = normalized_discounted_cumulative_gain(relevant_items, recommended_items, k)\n",
        "        avg_ndcg += ndcg_score\n",
        "\n",
        "    # Calcular promedio de MAP y NDCG para todos los usuarios\n",
        "    avg_precisionAtK = np.mean(precision_scores)\n",
        "    avg_map /= len(user_ids)\n",
        "    avg_ndcg /= len(user_ids)\n",
        "\n",
        "    return avg_precisionAtK, avg_map, avg_ndcg\n",
        "\n",
        "# Evaluar el modelo\n",
        "avg_precisionAtK, avg_map, avg_ndcg = evaluate_model(test_data.toPandas(), model, k=10)\n",
        "print(f\"Precision@10: {avg_precisionAtK:.4f}\")\n",
        "print(f\"Mean Average Precision (MAP)@10: {avg_map:.4f}\")\n",
        "print(f\"Normalized Discounted Cumulative Gain (NDCG)@10: {avg_ndcg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "udqPRpS_TVPW",
        "outputId": "487b6456-f861-436e-b84b-82692882f81c"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@10: 0.2463\n",
            "Mean Average Precision (MAP)@10: 0.0308\n",
            "Normalized Discounted Cumulative Gain (NDCG)@10: 0.1447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dando Recomendaciones"
      ],
      "metadata": {
        "id": "Q4lLNewkHJNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "By4quAMyHLr2",
        "outputId": "d95c9a86-a987-4a98-c363-22ff07862ad2"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------------+-------------+-----------------------+\n",
            "|user_id_index|artistname_index|reproductions|normalized_reproduction|\n",
            "+-------------+----------------+-------------+-----------------------+\n",
            "|          355|          258175|           58|   0.017040358744394617|\n",
            "|         8608|          258175|           42|   0.012257100149476832|\n",
            "|        12719|          258175|          138|    0.04095665171898356|\n",
            "|          550|          258175|          183|    0.05440956651718983|\n",
            "|         4052|          258175|           89|   0.026307922272047833|\n",
            "|        11906|          258175|           90|   0.026606875934230195|\n",
            "|        12659|          258175|          126|    0.03736920777279522|\n",
            "|         6366|          258175|           35|   0.010164424514200299|\n",
            "|         3222|          258175|           17|   0.004783258594917788|\n",
            "|          313|          258175|          108|    0.03198804185351271|\n",
            "|        13393|          258175|           47|   0.013751868460388639|\n",
            "|         2248|          258175|            5|   0.001195814648729447|\n",
            "|         8162|          258175|           56|   0.016442451420029897|\n",
            "|        13626|          258175|          113|    0.03348281016442452|\n",
            "|         2830|          258175|           14|   0.003886397608370...|\n",
            "|        14930|          258175|           12|   0.003288490284005979|\n",
            "|         3102|          258175|            2|   2.989536621823617...|\n",
            "|         6336|          258175|           47|   0.013751868460388639|\n",
            "|          261|          258175|           12|   0.003288490284005979|\n",
            "|        13235|          258175|           90|   0.026606875934230195|\n",
            "+-------------+----------------+-------------+-----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id_index=261\n",
        "#13235\n",
        "\n",
        "usr_reps = test_data.filter(fn.col(\"user_id_index\") == user_id_index).distinct()\\\n",
        "    .join(dfs['artistname'],'artistname_index').orderBy('reproductions', ascending=False).select('artistname','reproductions')\n",
        "\n",
        "n_artist_usr=usr_reps.count()\n",
        "usr_reps.show(n=n_artist_usr,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uWQ3lPjNHOTu",
        "outputId": "b538d548-7a16-44df-96d2-f131ea016afb"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-------------+\n",
            "|artistname             |reproductions|\n",
            "+-----------------------+-------------+\n",
            "|Grateful Dead          |42           |\n",
            "|The Smashing Pumpkins  |40           |\n",
            "|Iron Maiden            |31           |\n",
            "|Muse                   |16           |\n",
            "|Blur                   |16           |\n",
            "|Radiohead              |14           |\n",
            "|Coldplay               |13           |\n",
            "|U2                     |13           |\n",
            "|Arctic Monkeys         |12           |\n",
            "|Foo Fighters           |12           |\n",
            "|The Smiths             |12           |\n",
            "|The Rolling Stones     |12           |\n",
            "|Kings Of Leon          |12           |\n",
            "|David Bowie            |12           |\n",
            "|The Killers            |11           |\n",
            "|The Clash              |10           |\n",
            "|The Who                |9            |\n",
            "|Green Day              |9            |\n",
            "|Nirvana                |8            |\n",
            "|The Cure               |7            |\n",
            "|The Strokes            |7            |\n",
            "|R.E.M.                 |6            |\n",
            "|Weezer                 |6            |\n",
            "|Nine Inch Nails        |5            |\n",
            "|Guns N' Roses          |5            |\n",
            "|Beastie Boys           |5            |\n",
            "|Florence + The Machine |4            |\n",
            "|Daft Punk              |4            |\n",
            "|Linkin Park            |4            |\n",
            "|Pearl Jam              |3            |\n",
            "|Depeche Mode           |3            |\n",
            "|Arcade Fire            |3            |\n",
            "|The Doors              |3            |\n",
            "|Led Zeppelin           |3            |\n",
            "|Queens Of The Stone Age|3            |\n",
            "|Vampire Weekend        |2            |\n",
            "|Mumford & Sons         |2            |\n",
            "|Massive Attack         |2            |\n",
            "|The White Stripes      |2            |\n",
            "|Gorillaz               |2            |\n",
            "|blink-182              |2            |\n",
            "|Beck                   |2            |\n",
            "|Fall Out Boy           |2            |\n",
            "|Red Hot Chili Peppers  |1            |\n",
            "|Johnny Cash            |1            |\n",
            "|Various Artists        |1            |\n",
            "+-----------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generando 10 recomendaciones\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "\n",
        "rec_items = recommend_top_k_artists(user_id_index, loaded_model, train_data.toPandas(), 10)\n",
        "\n",
        "rec_items = [(int(item),) for item in rec_items]\n",
        "\n",
        "rec_artist_names = spark.createDataFrame(rec_items, StructType([StructField(\"artistname_index\", IntegerType(), True)])).join(dfs['artistname'],'artistname_index')\n",
        "\n",
        "print(f\"Recommended artists for user {user_id_index}\")\n",
        "rec_artist_names.show(n=rec_artist_names.count(),truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "q0UEBHTCHxSu",
        "outputId": "d670c086-330f-477c-b049-2f62db8f4259"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended artists for user 261\n",
            "+----------------+----------------------+\n",
            "|artistname_index|artistname            |\n",
            "+----------------+----------------------+\n",
            "|171812          |Miles Davis           |\n",
            "|168699          |Metallica             |\n",
            "|274942          |Vitamin String Quartet|\n",
            "|127024          |John Williams         |\n",
            "|258175          |The Smiths            |\n",
            "|100812          |Grateful Dead         |\n",
            "|269792          |U2                    |\n",
            "|100085          |Gorillaz              |\n",
            "|194927          |Pearl Jam             |\n",
            "|136060          |Kendrick Lamar        |\n",
            "+----------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Similutides:\n",
        "usr_reps.join(rec_artist_names,'artistname').select('artistname').show(n=n_artist_usr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2rcqPQCKH1uu",
        "outputId": "64a5439e-db1a-400a-907a-8c6d775ecb0c"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|   artistname|\n",
            "+-------------+\n",
            "|     Gorillaz|\n",
            "|Grateful Dead|\n",
            "|    Pearl Jam|\n",
            "|   The Smiths|\n",
            "|           U2|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = usr_reps.join(rec_artist_names,'artistname').count()/10\n",
        "\n",
        "print(f\"Precisi√≥n de las recomendaciones para el usuario {user_id_index}: {precision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UFOzCNAvH6_F",
        "outputId": "44c269a6-b8a1-425c-c16b-d14bad928fdd"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisi√≥n de las recomendaciones para el usuario 261: 0.5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}